---
title: "Construindo um web server em Assembly x86, parte II, hist√≥ria e arquitetura"
slug: "construindo-um-web-server-em-assembly-x86-parte-ii-historia-e-arquitetura-2jb9"
published_at: "2024-04-11 23:26:28Z"
language: "pt-BR"
status: "published"
tags: ["assembly", "braziliandevs"]
---

No [artigo anterior](https://dev.to/leandronsp/construindo-um-web-server-em-assembly-x86-parte-i-introducao-14p5) demos uma introdu√ß√£o n√£o-t√©cnica sobre o que ser√° esta saga de Assembly x86 conforme avan√ßamos na constru√ß√£o de um web server.

Agora, chegou o momento de come√ßarmos a de fato falar sobre coisas t√©cnicas. 

Como de costume, n√£o gosto de esgormitar termos complexos sem a devida explica√ß√£o. Portanto, vamos iniciar a saga trazendo um pouco de contexto hist√≥rico, motiva√ß√µes e porque estamos aqui quando o assunto √© **computadores**.

> Okay, agora fui fil√≥sofo demais. Mas o que mais importa √© que o verdadeiro Assembly s√£o os amigos que fazemos no caminho

Sem mais delongas, vamos ao que interessa.

---

## Agenda

* [Um pouco de hist√≥ria](#um-pouco-de-hist√≥ria)
* [Computar informa√ß√µes](#computar-informa√ß√µes)
  * [M√°quina de Turing](#m√°quina-de-turing)
  * [Arquitetura de von Neumann](#arquitetura-de-von-neumann)
  * [O gargalo de von Neumann](#o-gargalo-de-von-neumann)
* [Hierarquia de mem√≥ria](#hierarquia-de-mem√≥ria)
  * [Como a CPU executa instru√ß√µes](#como-a-cpu-executa-instru√ß√µes)
  * [Registradores de CPU](#registradores-de-cpu)
* [ISA](#isa)
  * [CISC](#cisc)
  * [RISC](#risc)
* [Por qu√™ x86?](#por-qu√™-x86)
* [Conclus√£o](#conclus√£o)
* [Refer√™ncias](#refer√™ncias)

---

## Um pouco de hist√≥ria
Ainda muitos milhares de anos a.C, o ser humano precisava realizar c√°lculos. Um dos instrumentos mais primitivos para esta tarefa era o √Åbaco, e certamente voc√™ j√° deve ter visto um:

![abaco](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/4rnhwceutwrg1kxc1amx.jpeg)

N√£o vou entrar em detalhes em como um √Åbaco funciona, sugiro que compre um e experimente. √â divertido. Eu usei quando estava no ensino prim√°rio (cria dos anos 90, cof cof).

### Computadores mec√¢nicos
Ainda nesta "pr√©-hist√≥ria" dos computadores e j√° avan√ßando para uma Europa iluminista (circa s√©culo XVII), podemos ver a seguir inven√ß√µes mec√¢nicas e projetos como a m√°quina de calcular de Blaise Pascal, depois a m√°quina anal√≠tica de Charles Babbage e ent√£o a m√°quina de tabula√ß√£o de Herman Hollerith.

![babbage machine](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/96zo1kt2of9iohmx3f02.jpeg)

> üí° √â da m√°quina de tabula√ß√£o de Herman Hollerith que vem o nome do seu comprovante de pagamento "holerite"

Estas m√°quinas eram mec√¢nicas e tinham muitas limita√ß√µes como n√£o ter uma mem√≥ria pr√≥pria para as "instru√ß√µes", mas foram muito importantes para a evolu√ß√£o, possibilitando mais tarde que Ada Lovelace pudesse escrever o primeiro poss√≠vel algoritmo para o projeto da m√°quina de Babbage.

> Pra quem quiser uma explica√ß√£o excelente e mais completa sobre a hist√≥ria dos computadores e como estes funcionam, sugiro o v√≠deo [como reinventar um computador do zero](https://www.youtube.com/watch?v=BbnDmeNojFA) do canal Infinitamente.

---

## Computar informa√ß√µes
Na era moderna dos computadores, que se d√° in√≠cio no s√©culo XX, √© quando acontece a revolu√ß√£o eletr√¥nica atrav√©s das v√°lvulas e dos transistores. 

Mas n√£o apenas na √°rea da eletr√¥nica. Foi no s√©culo XX que vimos a revolu√ß√£o computacional atrav√©s de um modelo abstrato que abriu portas para muito do que conhecemos hoje em termos de computadores.

Estamos falando da **m√°quina de Turing**.

### M√°quina de Turing
Nos anos 30, o matem√°tico Alan Turing desenvolveu o conceito abstrato de uma m√°quina que possu√≠a uma fita infinita, dividida em c√©lulas, e um cabe√ßote de **leitura/escrita** que movia para frente e para tr√°s na fita, possiblitando modificar o estado atual na m√°quina.


![turing machine](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/8hpp285hmrzc0txur45m.png)

Basicamente, este modelo de m√°quina permitiria que certas classes de problemas pudessem ser resolvidas com opera√ß√µes simples. 

Extrapolando, nesta fita, que pode representar um tipo de "mem√≥ria", √© poss√≠vel armazenar o estado mas tamb√©m outra m√°quina, ou seja, temos aqui o conceito de uma m√°quina de Turing universal, que √© capaz de simular outra m√°quina de Turing.

De forma resumida, podemos colocar na fita tanto o estado (dados) quanto as pr√≥prias instru√ß√µes do programa, mitigando assim o problema de limita√ß√£o que os computadores primitivos tinham, que era resolver problemas complexos de forma mais simples.

Entretanto, a m√°quina de Turing era apenas uma abstra√ß√£o. Este conceito de instru√ß√µes e estado na mesma mem√≥ria precisava ser concretizado.

_√â a√≠ que entra von Neumann_.

### Arquitetura de von Neumann
Von Neumman foi um pol√≠mata que prop√¥s um modelo computacional que √© utilizado por muitos computadores modernos e dispositivos que usamos hoje em dia.

Neste modelo, temos uma unidade de processamento central, ou CPU, que √© respons√°vel por realizar c√°lculos aritm√©ticos e executar instru√ß√µes.

Conectada a esta CPU, temos o conceito de _mem√≥ria_ compartilhada, que vai ser usada para armazenar todas as instru√ß√µes e estado de um programa de computador.


![von neumann 1](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/b0f759rhez1y6kinc7bf.png)

Esta arquitetura possibilitou que computadores como ENIAC e EDVAC pudessem ser desenvolvidos. O EDVAC, por sua vez, foi um dos precursores na implementa√ß√£o do modelo de von Neumann, com uma CPU que era conectada a uma mem√≥ria compartilhada e sequencial.

![edvac](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/vmlzomad54zpeapwc2jv.jpeg)

√Ä medida que os componentes computacionais foram ficando mais modernos, os computadores foram ficando menores, mais potentes, vers√°teis e com utiliza√ß√£o de prop√≥sito mais geral. 

Ent√£o, a arquitetura de von Neumann pode ainda contar com dispositivos de entrada e sa√≠da de dados (impressora, teclado, mouse, placa de rede, monitor, etc), tamb√©m conhecidos como dispositivos I/O:


![von neumann 2](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/fer0xexjvwrt4ooiwbjh.png)

Podemos assumir, ent√£o, que quando escrevemos um programa de computador, estamos basicamente manipulando uma mem√≥ria finita (que tem fim) e dispositivos de entrada e sa√≠da de dados, atrav√©s de instru√ß√µes que s√£o executadas pela CPU.

Tudo gra√ßas ao modelo concreto de von Neumann.

> Vale destacar que h√° arquiteturas computacionais que n√£o seguem este modelo, mas aqui neste guia estamos focando em computadores de prop√≥sito geral

### O gargalo de von Neumann
Esta arquitetura entretanto traz uma limita√ß√£o. Como o barramento (caminho) entre a CPU e mem√≥ria √© √∫nico, tanto instru√ß√µes quanto dados trafegam pelo mesmo local, levando a um cen√°rio onde a CPU pode ficar limitada em processamento at√© que todos os dados sejam lidos do barramento.

Uma forma de mitigar este problema √© definir diferentes "n√≠veis" de mem√≥ria, para que a CPU possa ter uma taxa de processamento maior.

Voc√™ acertou, vamos falar agora sobre a _hierarquia de mem√≥ria_.

---

## Hierarquia de mem√≥ria
Nosso programa manipula mem√≥ria. 

> Com que frequ√™ncia? 

Todo tempo.

Para mitigar o problema do gargalo de von Neumann, podemos definir uma hierarquia de mem√≥ria, assim n√£o s√≥ a mem√≥ria principal (RAM) √© "enxergada pela CPU" como mem√≥ria, mas tamb√©m outros dispositivos de armazenamento no sistema computacional.

Adicionado a isso, com a moderniza√ß√£o de computadores no s√©culo XX, foi criada a necessidade de orquestrar e controlar todas as interfaces com o hardware. Temos ent√£o a concep√ß√£o de **sistemas operacionais** para esta tarefa, que come√ßam a surgir em meados dos anos 60/70, dentre eles o UNIX.

Ao tratarmos tudo como mem√≥ria, podemos introduzir tal __hierarquia__. Portanto, num sistema computacional tratamos tudo (ou quase tudo) como mem√≥ria e assim o sistema operacional (SO) pode abstrair de um determinado programa onde aquilo na hierarquia se encontra de fato sendo utilizado, deixando ent√£o nosso programa "livre" deste detalhe de implementa√ß√£o f√≠sica.

![hierarquia memoria](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/i09gvyk1ext127w3u1mm.png)

Quanto mais pro topo da hierarquia, menor a capacidade de armazenamento e mais caro. Por exemplo, registradores de  CPU s√£o mem√≥rias vol√°teis que est√£o no topo.

> Vamos falar sobre registradores mais a seguir na saga

E quanto mais pra base da hierarquia, maior a capacidade e consequentemente mais barato. Exemplo na base s√£o as unidades de armazenamento dur√°vel n√£o-vol√°teis (HD, SSD etc).

No meio temos a mem√≥ria principal e vol√°til, tendo como principal exemplo a mem√≥ria RAM.

A hierarquia de mem√≥ria desempenha, ent√£o, um papel crucial na forma como a CPU gerencia e acessa mem√≥ria.

### Como a CPU executa instru√ß√µes
Para que uma CPU execute determinada instru√ß√£o, √© necess√°rio ao menos um ciclo de clock, tamb√©m chamado popularmente como "giro de CPU", ou _ciclo de CPU_.

Vamos imaginar uma coisa que fica "girando" indefinidamente igual um rel√≥gio. De forma simples, √© assim que podemos imaginar o clock de uma CPU, como um giro de rel√≥gio.

Alguns tipos de instru√ß√µes podem gastar mais de um ciclo, e determinar quais instru√ß√µes v√£o gastar mais ou menos ciclos √© algo que √© projetado diretamente na **constru√ß√£o da CPU**.

E onde as instru√ß√µes ficam armazenadas? 

> Isso mesmo, na mem√≥ria.

Portanto, a CPU precisa buscar a instru√ß√£o na mem√≥ria, decodificar, executar e armazenar o resultado de volta na mem√≥ria. 

Tudo isto faz gastar imensos ciclos de CPU. Ao gastarmos ciclos, a CPU pode bater num limite e n√£o conseguir atender a tantas opera√ß√µes no mesmo segundo. Pra n√£o mencionar a lat√™ncia que a CPU gasta pra utilizar o barramento f√≠sico e "viajar" at√© a mem√≥ria principal.

Tomando como premissa a hierarquia de mem√≥ria, e se ao inv√©s de armazenar o resultado na mem√≥ria principal, a CPU resolver armazenar dentro da pr√≥pria CPU?

Conhe√ßa os **registradores de CPU**.

### Registradores de CPU
Voc√™ j√° pode estar pensando em cache de CPU, n√©? Mas calma l√° jovem, cache de CPU √© outra seara que n√£o pretendo entrar, n√£o por agora.

> Mas que tamb√©m tem seu lugar na hierarquia de mem√≥ria

Lembrando (mais uma vez), da hierarquia de mem√≥ria, de forma muito simplificada:

* topo: registradores
* depois: cache de CPU
* ainda depois: mem√≥ria principal (RAM)
* beeem depois: mem√≥ria secund√°ria (HD, SSD)

Quanto mais perto do topo, mais r√°pido a CPU consegue processar, mas em contrapartida √© vol√°til e tamb√©m tem menor capacidade de armazenamento.

Ent√£o, registradores s√£o apenas mem√≥rias de hierarquia mais alta que s√£o preferencialmente usadas para computa√ß√£o porque possuem a menor lat√™ncia do conjunto de mem√≥rias dispon√≠veis.

S√£o nos registradores onde a CPU vai armazenar instru√ß√µes e dados do programa em execu√ß√£o, de modo a manipular sem precisar ficar dando tantos "saltos" na mem√≥ria principal, economizando assim lat√™ncia e ciclos de CPU.

![von neumann - registradores](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/1zzyclx72ff6nyr7b3ag.png)

Vamos pensar nos registradores como "caixinhas" de tamanho fixo que ficam dentro da CPU. 

E como podemos manipular os registradores da CPU? Devemos estabelecer um padr√£o, que define como controlar o **conjunto de instru√ß√µes** da CPU.

Precisamos de uma arquitetura de conjunto de instru√ß√µes, ou **ISA** (_Instruction Set Architecture_).

---

## ISA
ISA serve para definir o conjunto instru√ß√µes e registradores em uma determinada CPU. O fabricante define a ISA, que pode ser classificada de formas diferentes, determinando quantas opera√ß√µes podem ser feitas por instru√ß√£o, entre outros aspectos.

Neste artigo vamos destacar 2 abordagens de conjunto de instru√ß√µes: _CISC e RISC_.

### CISC
CISC, ou **Complex Instruction Set Computing**, √© uma arquitetura onde as instru√ß√µes podem ser agrupadas em conjuntos mais complexos de instru√ß√µes, permitindo que uma √∫nica instru√ß√£o execute v√°rias opera√ß√µes complexas.

Aqui, determinadas tarefas podem resultar em apenas um ciclo de CPU, o que aumenta efici√™ncia, mas por outro lado, esta complexidade de instru√ß√µes pode tornar o tempo de execu√ß√£o menos previs√≠vel.

Exemplos de arquiteturas CISC incluem System/360, PDP-11 e Intel 8086.

### RISC
Para resolver o problema de instru√ß√µes muito complexas em CISC, a arquiteura RISC, ou **Reduced Instruction Set Computing**, determina uma execu√ß√£o mais simples com menos instru√ß√µes, diminuindo assim o n√∫mero de circuitos e consequentemente ciclos de CPU. O tamanho das instru√ß√µes geralmente √© fixo, resultando em um desempenho mais r√°pido e previs√≠vel.

Exemplos de arquiteturas RISC s√£o MIPS e ARM, sendo ARM atualmente utilizada nos processadores de MacBook M1 em diante.

Este √© um dos motivos de um MacBook ARM consumir menos bateria e fazer menos barulho, por exemplo.

### E j√° que o tema √© x86...
Como a saga se trata de x86, especificamente 64-bit (logo mais vamos entender o motivo disso), inicialmente esta arquitetura foi desenvolvida seguindo o padr√£o CISC, que √© o conjunto complexo de instru√ß√µes.

Entretanto a ISA do x86 foi adaptada para suportar internamente opera√ß√µes simplificadas como encontramos em RISC, portanto pode-se sizer que x86 √© um "fake-CISC", que segue um modelo "RISC-ish".

---

## Por qu√™ x86?
Alguns leitores mais atentos devem estar se perguntando: por qu√™ raios x86? O que isto significa?

Bom, pra entender o que √© isto, vamos mergulhar um pouco na hist√≥ria dos microprocessadores da Intel.

### Anos 70
Os anos 70 foram primordiais. Al√©m da explos√£o cambriana de sistemas operacionais, vemos tamb√©m a consolida√ß√£o da era dos transistores e assim a evolu√ß√£o das CPU's. 

Do lado Intel, temos o 8080 de 8-bit lan√ßado em 1974, que al√©m de ser utilizado em sistemas industriais, tamb√©m foi amplamente encontrado nos primeiros computadores pessoais.

### Intel 8086
Esta vers√£o traz consigo conjuntos de instru√ß√µes de 16-bits e foi um marco na era dos computadores pessoais. √â aqui que passa a ser cunhado o termo de fam√≠lia "x86", pois o "x" caracteriza qualquer n√∫mero que venha antes de "86".

Intuitivo, n√£o?

### Anos 80, a d√©cada do Intel x86 
A partir de ent√£o, nesta d√©cada vimos a chegada do 80286 (286) que suporta tamb√©m 16-bit + 8-bit de endere√ßamento de mem√≥ria; depois, o famoso 80386 (i386), que trouxe uma grande mudan√ßa suportando instru√ß√µes de 32-bit e os famosos registradores `exx` (eax, ebx, eip, etc); para ent√£o chegarmos ao 80486 (486), que foi uma melhoria do 386 com suporte a instru√ß√µes mais avan√ßadas.

> Pra quem tiver curiosidade em saber a especifica√ß√£o da arquitetura x86, est√° tudo bem documentado num simples [manual de 5000 p√°ginas](https://software.intel.com/en-us/download/intel-64-and-ia-32-architectures-sdm-combined-volumes-1-2a-2b-2c-2d-3a-3b-3c-3d-and-4)

### Anos 90, Pentium e al√©m
O que vem a seguir √© a evolu√ß√£o seguindo com os Pentium 586, 686 e depois, uma simplifica√ß√£o dos termos para x86_32 (32-bit) ou x86_64 (64-bit).

Destaca-se os Pentium II, III e os Core "i AlgumaCoisa" que perduram at√© hoje.

> E os AMD?

Enquanto a Intel dominava o mercado de CPU's, a AMD (Advanced Micro Devices) mexeu os palitinhos e tamb√©m lan√ßou vers√µes compat√≠veis com x86, portanto pode-se dizer que, ao desenvolver para arquitetura x86, √© poss√≠vel executar as mesmas instru√ß√µes em uma CPU fabricada pela AMD.

---

## Conclus√£o
√â isto. Este foi um artigo bastante denso, que cobriu uma breve hist√≥ria dos computadores, passando por modelos computacionais at√© chegar nas arquiteturas de CPU's e entendermos o que significa aquele "x86" no t√≠tulo do artigo.

No pr√≥ximo artigo, pretendo trazer brevemente sistema bin√°rio e hexadecimal para ent√£o come√ßar a apresentar aquilo que estamos todos interessados: linguagem de montagem, ou simplesmente **Assembly**.

_Artigo revisado com carinho por [Jeff Quesado](https://twitter.com/JeffQuesado), o "Coelho da Bolha", e tamb√©m por [Cadu](https://twitter.com/_____cadu_____), o m√∫sico frustrado (same thing)._

---

## Refer√™ncias

<sub>
√Åbaco, Wikipedia
https://pt.wikipedia.org/wiki/%C3%81baco
M√°quina anal√≠tica, Wikipedia
https://pt.wikipedia.org/wiki/M%C3%A1quina_anal%C3%ADtica
Linguagem Assembly, Wikipedia
https://pt.wikipedia.org/wiki/Linguagem_assembly
Cronologia do x86, Wikipedia
https://pt.wikipedia.org/wiki/X86
Arquitetura de Von Neumann, Wikipedia
https://pt.wikipedia.org/wiki/Arquitetura_de_von_Neumann
Blau Araujo, "Fundamentos de Assembly com NASM"
https://codeberg.org/blau_araujo/assembly-nasm-x86_64
Hist√≥ria da Computa√ß√£o, Wikipedia
https://pt.wikipedia.org/wiki/Hist%C3%B3ria_da_computa%C3%A7%C3%A3o
Inifitamente, "Como reinventar um computador do zero"
https://www.youtube.com/watch?v=BbnDmeNojFA
Total Phase, "What's a CPU register"
https://www.totalphase.com/blog/2023/05/what-is-register-in-cpu-how-does-it-work/
Turing Machine, Wikipedia
https://en.wikipedia.org/wiki/Turing_machine
Brilliant, "Turing Machines"
https://brilliant.org/wiki/turing-machines/
Instruction Set Architecture, Wikipedia
https://en.wikipedia.org/wiki/Instruction_set_architecture
</sub>
